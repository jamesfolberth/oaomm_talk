\documentclass[xcolor=dvipsnames,t]{beamer} % position stuff on top of slide
% Functions, packages, etc.
%[[[
\usetheme{Frankfurt}
\usecolortheme[named=Maroon]{structure}

% Remove Navigation Symbols
\usenavigationsymbolstemplate{}

\usepackage{mathdots} % for \iddots

\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{calc}
\tikzset{node style ge/.style={circle}}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}

\usepackage{tikz,gnuplot-lua-tikz}
%\usetikzlibrary{external} % speed up TikZ compilation by caching figures; use lualatex over pdflatex (dynamic memory alloc)
%%XXX need to add -enable-write18 to initial lualatex call in Makefile
%\tikzexternalize[prefix=tikz_cache/]
%\tikzset{external/system call={lualatex
%      \tikzexternalcheckshellescape -halt-on-error -interaction=batchmode
%-jobname "\image" "\texsource"}}%

\usepackage{graphicx}
%\usepackage{subfig}
\usepackage[labelfont=bf]{caption}
%\usepackage[labelfont=bf]{subcaption}
%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
%\pagenumbering{arabic}
\usepackage{hyperref}
%\numberwithin{equation}{section}
%\usepackage{soul} % for \ul - a ``better'' underlining command

%\usepackage{colortbl} % for coloring \multicolumn (tabular in general, I think)
% For \rowcolor
%\definecolor{table_header}{gray}{0.5}
%\definecolor{table_data}{gray}{0.85}

\usepackage[Algorithm,ruled]{algorithm}
\usepackage{algpseudocode,algorithmicx}
%\algnewcommand\algorithmicinput

%% Inserting code and syntax highlighting
% [[[
\usepackage{listings} % like verbatim, but allows for syntax highlighting and more
\usepackage{color} % colors
%\usepackage[usenames,dvipsnames]{xcolor}% More colors
\usepackage{caption} % captions
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\usepackage{framed} % put a frame around things

% define some custom colors
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{lgreen}{rgb}{0.25,1,0}
\definecolor{purple}{rgb}{0.35,0.02,0.48}

% Some changes to MATLAB/GNU Octave language in listings
\lstset{frame=tbrl,
    language=Matlab,
    aboveskip=3mm,
    belowskip=3mm,
    belowcaptionskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\scriptsize\ttfamily\color{black}},
    numbers=left,
    numberstyle=\tiny\color{purple},
    keywordstyle=\color{dkgreen},
    commentstyle=\color{red},
    stringstyle=\color{purple},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    rulecolor=\color{black},
    morekeywords={string,fstream}
}
% ]]]


%My Functions
\newcommand{\diff}[2]{\dfrac{d #1}{d #2}}
\newcommand{\diffn}[3]{\dfrac{d^{#3} #1}{d {#2}^{#3}}}
\newcommand{\pdiff}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\pdiffn}[3]{\dfrac{\partial^{#3} #1}{\partial {#2}^{#3}}}
\newcommand{\problemline}{\rule{\textwidth}{0.25mm}}
%\newcommand{\problem}[1]{\problemline\\#1\\\problemline\vspace{10pt}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\qline}[2]{\qbezier(#1)(#1)(#2)}
\newcommand{\abox}[1]{\begin{center}\fbox{#1}\end{center}}
\newcommand{\lie}{\mathcal{L}}
%\newcommand{\defeq}{\stackrel{\operatorname{def}}{=}}

% AMS theorem stuff
% [[[
%\newtheoremstyle{mystuff}{}{}{\itshape}{}{\bfseries}{:}{.5em}{}
%\theoremstyle{mystuff}
%\newtheorem{definition}{Definition}[section]
%\newtheorem*{definition*}{Definition}
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem*{theorem*}{Theorem}
%\newtheorem{lemma}{Lemma}[section]
%\newtheorem*{lemma*}{Lemma}
%\newtheorem{proposition}{Proposition}[section]
\newtheorem*{proposition*}{Proposition}
%\newtheorem*{corollary*}{Corollary}
%\newtheorem*{remark}{Remark}
%
%\newtheoremstyle{myexample}{}{}{}{}{\bfseries}{:}{.5em}{}
%\theoremstyle{myexample}
%\newtheorem*{example*}{Example}
%
%% Stolen from http://tex.stackexchange.com/questions/8089/changing-style-of-proof
%\makeatletter \renewenvironment{proof}[1][\proofname] {\par\pushQED{\qed}\itshape\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{:}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother
%
%% Stolen from http://tex.stackexchange.com/questions/12913/customizing-theorem-name
%\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{:}{.5em}{\thmnote{#3's }#1}
%\theoremstyle{named}
%\newtheorem*{namedtheorem}{Theorem}
% ]]]
%]]]


% Output Control Variables
\def\true{1}
\def\false{0}
\def\figures{1}
\def\tables{1}

\newcommand{\drm}{\mathrm{d}}
\renewcommand{\UrlFont}{\scriptsize}
\newcommand{\defeq}{\mathrel{\mathop:}=}
\DeclareMathOperator*{\argmin}{arg\,min}

% http://tex.stackexchange.com/questions/43065/matrices-too-big-to-fit-the-width-of-a-page
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

% Taken from _somewhere_ in Stephen's slides
% Adds outline slide before each section
\AtBeginSection[]
%\AtBeginSubsection[]
{\begin{frame}
        \frametitle{Outline}
        \small
        \tableofcontents[currentsection,hidesubsections,subsectionstyle=hide]
%        \tableofcontents[currentsection,%]%,% % normal one
%        currentsubsection]
        %hideothersubsections, 
        %sectionstyle=show/shaded, 
        %%sectionstyle=show/hide, 
        %subsectionstyle=show/shaded, 
        %]
%        \tableofcontents[currentsection] % before Nov 2015, used this
        %\tableofcontents[currentsubsection]
        \normalsize
        \addtocounter{framenumber}{-1}
    \end{frame}}

% From http://tex.stackexchange.com/questions/30720/footnote-without-a-marker
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\title{A Brief Introduction to Optimization Algorithms on Matrix Manifolds}
\date{11 October, 2016}
\author{James Folberth} %TODO add Stephen as advisor?  Don't want people to think this is our work!
\institute{University of Colorado at Boulder}

% Abstract: (no word wrap)
%Certain optimization problems have special, geometric constraints, which specify that the solution of the optimization problem must lie on a manifold.  The recent text "Optimization Algorithms on Matrix Manifolds", by P.-A. Absil, R. Mahoney, and R. Sepulchre (2008), develops the necessary machinery from differential geometry and discusses various problems on matrix manifolds, with an emphasis on computing invariant subspaces of matrices.  The authors develop gradient descent, Newton's method, trust-region methods, and other high-order methods.  This talk will highlight some of the tools required to pose gradient descent on matrix manifolds, demonstrate a few examples in Manopt, a MATLAB toolbox for optimization on manifolds, and discuss a few recent results.  None of this talk is my own work, but I hope to convey some of the excitement in this field which is quickly gaining momentum.

% Abstract:
%Certain optimization problems have special, geometric constraints, which
%specify that the solution of the optimization problem must lie on a manifold.
%The recent text "Optimization Algorithms on Matrix Manifolds", by P.-A. Absil,
%R. Mahoney, and R. Sepulchre (2008), develops the necessary machinery from
%differential geometry and discusses various problems on matrix manifolds, with
%an emphasis on computing invariant subspaces of matrices.  The authors develop
%gradient descent, Newton's method, trust-region methods, and other high-order
%methods.  This talk will highlight some of the tools required to pose gradient
%descent on matrix manifolds, demonstrate a few examples in Manopt, a MATLAB
%toolbox for optimization on manifolds, and discuss a few recent results.  None
%of this talk is my own work, but I hope to convey some of the excitement in
%this field which is quickly gaining momentum.


\begin{document}

\begin{frame}
\maketitle
\end{frame}

%TODO should we have a section name slide (e.g. a slide with a big "A Motivating Example" in the middle)?
%TODO CU colors?  I asked Stephen before, but didn't follow through
%TODO Riemann Newton's connection to Jacobi-Davidson

\begin{frame}{A Recent Textbook}
% [[[
   Most of the material in this talk is from \emph{Optimization Algorithms on Matrix Manifolds}, P.-A. Absil, R. Mahoney, R. Sepulchre, Princeton University Press, 2008.\\
   
   \begin{columns}
      \begin{column}{0.4\textwidth}
         \begin{center}
            \includegraphics[width=0.8\columnwidth]{figures/oaomm_cover.png}
         \end{center}

      \end{column}
      \begin{column}{0.6\textwidth}
         {\footnotesize
         \begin{enumerate}[1.]
            \item Introduction
            \item Motivation and Applications
            \item Matrix Manifolds: First-Order Geometry
            \item Line-Search Algorithms on Manifolds
            \item Matrix Manifolds: Second-Order Geometry
            \item Newton's Method
            \item Trust-Region Methods
            \item A Constellation of Superlinear Algorithms
         \end{enumerate}
         }
      \end{column}
   \end{columns}
% ]]]
\end{frame}


\section{A Motivating Example}
% [[[
\begin{frame}{Extreme Eigenspace Computation}
   Consider a symmetric matrix $A\in\reals^{n\times n}$ with eigenvalues $\lambda_1\le \cdots \le \lambda_n$.

   \begin{proposition*}
      Consider the \emph{generalized Rayleigh quotient}
      
      %NOTE f(Y) depends only of span(Y), not on each individual column;
      %     This a fundamental reason for optimizing of Grass(p,n)
      \[ f:\reals^{n\times p}_\ast \to \reals : Y \mapsto f(Y) = \operatorname{tr}(Y^TAY(Y^TY)^{-1}) \] 

      \noindent defined on $\reals^{n\times p}_\ast$, the set of all real $n\times p$ full-rank matrices.\\

      TFAE:
      \begin{enumerate}[1.]
         \item $Y_\ast$ is a global minimizer of $f(Y)$ over $\reals^{n\times p}_\ast$;
         \item $\operatorname{span}(Y_\ast)$ is a leftmost invariant subspace of $A$;
         \item $f(Y_\ast) = \sum_{i=1}^p \lambda_i$.
      \end{enumerate}
   \end{proposition*}

\end{frame}


\begin{frame}{Rayleigh Quotient}
   For simplicity, take $p=1$ and assume $\lambda_1$ has multiplicity 1.\\

   \[ f: \reals^n_\ast \to \reals: y\mapsto f(y) = \dfrac{y^TAy}{y^Ty}. \] 

   %NOTE no interesting manifold aspects; \reals^n is a linear manifold (in fact, a Euclidean space)
   \noindent $\reals^n_\ast$ is $\reals^n$ with the origin removed\footnote{$\reals_\ast^{n\times p}$ - full-rank, $n\times p$ matrices}.\\[.5em]
   
   %NOTE Critical points of $f$ correspond to eigenvalues of $A$.
   
   \pause
   \noindent The minimizers of $f(y)$ are \emph{not} isolated: they appear as the continuum $v_1\reals_\ast = \{v_1r : 0\neq r\in\reals\}$.\\[.5em]

   \noindent Consequently, important optimization algorithms may not apply.\\[.5em]
   
   %OLD\noindent Newton's method applied to the Rayleigh quotient yields the iteration $y\mapsto 2y$ for every non-critical $y$.
   \noindent For every non-critical $y$, Newton's method gives

   \[ y\mapsto 2y. \] 

\end{frame}


%TODO want to put Embedded submanifold, Quotient manifold.  How to format?
%\begin{frame}{Rayleigh Quotient} % no nonsense
\begin{frame}{Rayleigh Quotient - The $\mathcal{M}$ Word} % nonsense
   \setbeamercovered{transparent}
   \begin{columns}
      \begin{column}{0.5\textwidth}
         \uncover<1>{A standard remedy is to restrict the Rayleigh quotient to the unit sphere

   \[ S^{n-1}\defeq \left\{y\in\reals^n \,:\, y^Ty=1\right\}. \]
   
   %\noindent Minimizers are now isolated, however the domain of $f$ is now nonlinear.\\[.5em]
   \noindent Minimizers are now isolated, and we must work on the sphere.\\[.5em]
   %NOTE but that's okay, because that's the whole point of the book.

   %NOTE This takes an unconstrained problem and adds a constraint.  We handle the constraint
   %     by incorporating the constraint into the geometry of the domain.
   
   \begin{center}
      ~\\[1.3em] %HACK
      \color{red}{Embedded submanifold}
   \end{center}
}
      \end{column}

      \begin{column}{0.5\textwidth}
         %\uncover<2>{Consider all points on a (two-sided) ray $y\reals_\ast$\footnotemark[1] as a single point.  This leads us to use the domain
         \uncover<2>{Treat all points on a two-sided ray $y\reals_\ast$\footnotemark[1] as a single point (equivalence class).

   \[ \mathcal{M} = \left\{y\reals_\ast \,:\, y\in\reals^n_\ast\right\}, \] 

   \noindent the set of all 1-dimensional subspaces of $\reals^n$.\\[.5em]
   %NOTE Each point of M is now an equivalence class of points in \reals^n_\ast.
   %     In other words, M is the set of all 1-dimensional subspaces of \reals^n
   %     This set admits the structure of a quotient manifold; it is, in fact, RP^{n-1}
    
   %UNUSED Since $f(y\alpha)=f(y)$ $\forall \alpha \neq 0$, $f$ induces a well-defined function on $\mathcal{M}$.
   Minimizers are isolated, but points are now equivalence classes.

   \begin{center}
      \color{red}{Quotient manifold}
   \end{center}
}
      \end{column}
   \end{columns}

   \uncover<2>{\footnotetext[1]{$\reals_\ast^{n\times p}$ - full-rank, $n\times p$ matrices}}
\end{frame}

%TODO how to wrap up Rayleigh quotient example?
%     Lead into manifolds


\begin{frame}{Some Benefits}
   
   For optimization-based eigenvalue algorithms:\\
   \begin{itemize}
      \item Solid framework for convergence analysis.  Many algorithms exhibit almost global convergence.
      \item Speed of convergence is intrinsic to the algorithm.  Gradient-based is linear; Newton-like are superlinear.
      \item Matrix-free methods, so $A$ is used only as an operator $x\mapsto Ax$.
   \end{itemize}
\end{frame}

% ]]]

%TODO all this should go in a slide?  Maybe at the end?
%NOTE what does additional geometric structure bring us?
%     Convergence analysis is nice (optimization)
%     Convergence speed is nice (gradient -> linear, higher-order -> superlinear)
%     Matrix-free methods

\section{Manifolds}
% [[[
   \begin{frame}{What is a Smooth Manifold?}
   %OLD Roughly, a topological space that locally resembles $\reals^d$ near each point.\\[0.5em]
   \emph{Very roughly}, a (possibly) non-Euclidean space that locally resembles Euclidean space.\\[.5em]

   %NOTE We want all the charts (U,\varphi) to vary smoothly (C^\infty is a standard choice)
   %     Basically mapping open sets of M smoothly to open sets of R^n.
   %     A maximal atlas of "nice+smooth" charts is a differentiable structure on M
   %NOTE Mention that we're only going to work with smooth manifolds
   \begin{center}
      \includegraphics[width=0.9\textwidth]{figures/charts.pdf}
   \end{center}
\end{frame}


\begin{frame}{Some Common Manifolds}
   \begin{description}
      \item[$\reals^n,\reals^{n\times p}$] Linear manifold viewed as vector space endowed with inner product (a Euclidean space).
      \item[Sphere] $S^{n-1}\defeq\left\{x\in\reals^n\,:\,x^Tx=1\right\}$.
         \begin{center}
            \includegraphics[width=.3\textwidth]{figures/sphere_with_chart.png}
         \end{center}
   \end{description}
\end{frame}

\begin{frame}{Some Common Manifolds}
   \setbeamercovered{transparent}
   \begin{description}
      \uncover<1>{\item[Stiefel Manifold] $\mathrm{St}(p,n) \defeq \left\{X\in\reals^{n\times p}\,:\,X^TX=I_p\right\}$\\$X$ is tall-skinny, has orthonormal columns
         $St(1,n) = S^{n-1}$\\
         $St(n,n) = O_n$\\[.5em]
         Applications in computer vision, PCA, ICA.\\[1em]}
   
      \uncover<2>{\item[Grassmann Manifold] $\text{Grass}(p,n)\simeq \reals_\ast^{n\times p}/ \text{GL}_p$\\
         The set of all $p$-dimensional subspaces in $\reals^n$.\\
         %NOTE Can identify $\mathrm{Grass}(p,n)$ with $\reals^{n\times p}_\ast\setminus\sim$, where
         %\[ X\sim Y \leftrightarrow \operatorname{span}(X)=\operatorname{span}(Y). \] 
         
         $\text{Grass}(1,n) = \reals\mathbb{P}^{n-1}$\\
         $\text{Grass}(n,n) = \text{GL}_n$\\[.5em]
         Applications in various dimension reduction problems.}
   \end{description}

   \uncover<2->{There are many more (included in Manopt).}
   
   \vspace{1em}
   \uncover<2->{\footnotesize $\reals_\ast^{n\times p}$ - full-rank, $n\times p$ matrices}
\end{frame}



\begin{frame}{Tangent Vectors}
   Let $\gamma:\reals\to\mathcal{M}$ be a curve on $\mathcal{M}$ s.t. $\gamma(0)=x$.\\[.5em]
   Can't always define tangent vectors as 

   \[ \gamma'(0) = \lim_{\tau\to 0} \dfrac{\gamma(\tau)-\gamma(0)}{\tau}, \] 
   
   %NOTE And if M is linear, what are we even doing here?
   %     Be sure to _really_ point out that \gamma(t) \in M, and that we don't
   %     (always) have a linear structure on M.
   \noindent as this requires a vector space structure on $\mathcal{M}$.
    
   %NOTE Here we visualize an embedded submanifold of a vector space \mathcal{E}.  The vector v in 
   %     the figure corresponds to $\gamma'(0)$.  Roughly, \dot{\gamma}(0)f = Df(x)[\gamma'(0)]$.
   \begin{center}
      \includegraphics[width=0.7\textwidth]{figures/tangent_space.pdf}
   \end{center}
\end{frame}


\begin{frame}{Tangent Vectors}
   A tangent vector $\xi_x$ is a mapping, realized by $\gamma$, defined by %OLD taking $f$ to $\reals$ defined by
   
   %NOTE This definition makes sense because we can do f(gamma(tau)) - f(gamma(0)),
   %     since R is a vector space
   %     Also, there are other ways of defining tangent vectors (e.g. derivations)
   \[ \xi_x f = \dot{\gamma}(0)f \defeq \dfrac{\drm(f(\gamma(t)))}{\drm t}\biggr|_{t=0}, \] 
   
   \noindent for all smooth $f:\mathcal{M}\to\reals$.\\

   %NOTE Be sure to mention that the tangent space admits a vector space structure.
   %     One can think of the tangent space as a local, vector space approximation to M
   \noindent The set of all tangent vectors based at $x$ is the tangent space $T_x\mathcal{M}$.  Each tangent space is a \emph{vector space}.  %TODO Do we need this: The tangent bundle is $\cup_{x\in\mathcal{M}}T_x\mathcal{M}$.

   \begin{center}
      \includegraphics[width=0.7\textwidth]{figures/tangent_space.pdf}
   \end{center}
\end{frame}


\begin{frame}{Tangent Vectors}
   Tangent vectors are used to generalize the directional derivative,

   \[ Df(x)[\xi_x] = \lim_{t\to 0} \dfrac{f(x+t\xi_x)-f(x)}{t}, \] 
   
   \noindent where $x+t\xi_x$ only makes sense for vector spaces.\\[.5em]

   \noindent More generally, if $\xi_x$ is realized by $\gamma(t)$,

   \begin{align*} Df(x)[\xi_x] &= \xi_xf\\
                               &= \dot{\gamma}(0)f = \dfrac{\drm(f(\gamma(t)))}{\drm t}\biggr|_{t=0}.
                                 \end{align*}

\end{frame}


\begin{frame}{Riemannian Metric}
   \setbeamercovered{transparent}
   \uncover<1->{A Riemannian metric is a smoothly varying \emph{inner product} on the tangent spaces of $\mathcal{M}$:
   
   %NOTE The Riemannian metric can be induced from the ambient space (e.g. R^{n x p}), or somewhere else
   %     The notation g_x is just to indicate that the metric generally depends on x\in M
   \[ g(\xi_x,\zeta_x) = g_x(\xi_x,\zeta_x). \] 

   A Euclidean space $\mathcal{E}$ is a particular \emph{Riemannian manifold}.\\}

   \uncover<2->{A Riemannian metric allows us to compute}
   \begin{itemize}
      \uncover<2->{\item lengths

         \[ L(\gamma) = \int_a^b \sqrt{g(\dot{\gamma}(t),\dot{\gamma}(t))}\drm t. \] 
      }
   \uncover<3->{\item distances (a metric space \emph{metric})
         
         \[ \operatorname{dist}:\mathcal{M}\times\mathcal{M}\to\reals:\operatorname{dist}(x,y)=\inf_{\Gamma_{x\to y}} L(\gamma) \] 
         }
   \end{itemize}
\end{frame}

\begin{frame}{Riemannian Metric}
   \begin{itemize}
      %NOTE emphasize that this is the "Riemannian" gradient, but one can compute it from the "Euclidean" gradient
      \item gradients: The unique element of $T_x\mathcal{M}$ that satisfies
         
         %NOTE Maybe write on the board Calc III style
         %     D_\hat{u}f(x) = \nabla f(x) \cdot \hat{u}
         \[ g_x(\operatorname{grad}f(x),\xi) = Df(x)[\xi], \quad \forall \xi\in T_x\mathcal{M}. \] 

         \noindent Can compute the Euclidean gradient $\nabla \bar{f}(x)$ and convert to Riemannian gradient $\operatorname{grad}f(x)$.

   \end{itemize}

   \begin{center}
      \includegraphics[width=\textwidth]{figures/gradient.pdf}
   \end{center}
\end{frame}

% ]]]


\section{Gradient Descent}
% [[[
\begin{frame}{Gradient Descent on $\reals^n$}
   Consider gradient descent on $\reals^n$.  Initialize $x\in\reals^n$ and iterate

   \[ x_{+}=x - t\nabla f(x). \] 

   \noindent The idea is simple: head in the locally most promising direction ($-\nabla f(x)$) for an appropriate distance.\\[0.5em]

   \noindent Line-search over the curve $\gamma(t) = x - t\nabla f(x)$.

   %To generalize to a manifold $\mathcal{M}$, we need a few things:
   %\begin{itemize}
   %   \item To move away from $x_k$, we need to stay on $\mathcal{M}$.
   %   \item Locally most promising direction
   %\end{itemize}

   \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/GD_Rn.pdf}
   \end{center}

\end{frame}

\begin{frame}{Gradient Descent on $\mathcal{M}$}

   Head in the direction of $-\operatorname{grad}f(x)$.\\[0.5em]

   \noindent Line-search over a curve s.t. $\gamma(0)=x$ and $\dot{\gamma}(0)=-\operatorname{grad}f(x)$.\\[0.5em]

   \noindent We need a method to cheaply find suitable curves (geodesics work, but are generally expensive to find).\\
       
   \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/GD_M.pdf}
   \end{center}

\end{frame}

\begin{frame}{Retractions}
   
   A \emph{retraction} $R:T\mathcal{M}=\bigcup_x T_x\mathcal{M}\to\mathcal{M}$ satisfies
   \begin{enumerate}[1.]
      \item $R(0_x) = x$, where $0_x$ is the zero element of $T_x\mathcal{M}$
      \item $\diff{}{t}R(t\xi_x)\biggr|_{t=0} = \xi_x \quad \forall \xi_x\in T_x\mathcal{M}$.
   \end{enumerate}
   
   %NOTE Item 2 is a slight abuse of notation.  Consider the curve gamma(t) = R(t\xi).
   %     Then we should have \dot{\gamma}(0)=\xi

   %NOTE compare to Exp_x, which is nice theoretically, but expensive to compute.
   %     Retractions should be a cheaper alternative, unless geodesics have a nice closed form
   %     (e.g. PD matrices)
      
   \begin{center}
      \includegraphics[width=0.7\textwidth]{figures/retraction.pdf}
   \end{center}
\end{frame}

\begin{frame}{Retractions}
   
   Equipped with a retraction $R$, the curve $\gamma(t) = R(-t\operatorname{grad}f(x))$ satisfies
   \begin{enumerate}[1.]
      \item $\gamma(0)=x$;
      \item $\dot{\gamma}(0)=-\operatorname{grad}f(x)$.
   \end{enumerate}
   %NOTE compare to Exp_x, which is nice theoretically, but expensive to compute.
   %     Retractions should be a cheaper alternative, unless geodesics have a nice closed form
   %     (e.g. PD matrices)
     
   Along the curve, $f(\gamma(t))$ is a smooth function from $\reals$ to $\reals$.  Can use standard line-search methods (e.g. Armijo backtracking).

   \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/GD_M.pdf}
   \end{center}
  
\end{frame}


\begin{frame}{Gradient Descent for the Rayleigh Quotient on $S^{n-1}$}
   \setbeamercovered{transparent}
   \uncover<1>{Consider the sphere $S^{n-1}$.  We can view $x\in S^{n-1}$ as an element of $\reals^n$, and $\xi\in T_xS^{n-1}$ as an element of $T_x\reals^n\simeq \reals^n$.\\[.5em]

   %NOTE this retraction is sometimes called the projective retraction, since it is 
   %     a projection radially down onto the sphere.
   A suitable retraction on $S^{n-1}$ is the projection/renormalization:

   \[ R_x(\xi) = \dfrac{x+\xi}{\|x+\xi\|}. \] 
   }
   
   \uncover<2>{Consider the Rayleigh quotient on the sphere:
   
   \[ f:S^{n-1}\to \reals: x\mapsto f(x) =x^TAx. \] 

   It is easy to show
   
   \[ \operatorname{grad}f(x)=(I-xx^T)2Ax = 2(Ax-xx^TAx). \] 
   }
\end{frame}


\begin{frame}{Gradient Descent for the Rayleigh Quotient on $S^{n-1}$}
   
   %NOTE if we take the step t_k = 1/(2x_k^TAx_k), we recover the power method
   \begin{algorithm}[H]
      \caption{Armijo line search for the Rayleigh quotient on $S^{n-1}$}
      \begin{algorithmic}[1] % the [1] adds line numbers
         \Require Symmetric matrix $A$, Armijo backtracking parameters $\overline{\alpha}>0,\beta,\sigma\in(0,1)$.
         \item[\textbf{Input:}] Initial iterate $x_0$ s.t. $\|x_0\|=1$. %XXX hacking algorithmicx package a bit
         \item[\textbf{Output:}] Sequence of iterates $\{x_k\}$.
         \For{k=0,1,2,...}
            %\State Gradient: compute $\eta_k = -\operatorname{grad} f(x_k) = -2(Ax_k-x_kx_k^TAx_k).$
            \State Gradient: compute ${\eta_k = -2(Ax_k-x_kx_k^TAx_k)}.$
            \State Backtrack: find the smallest integer $m\ge 0$ such that
            \[ f(R_{x_k}(\overline{\alpha}\beta^m\eta_k))\le f(x_k) - \sigma\overline{\alpha}\beta^m\eta_k^T\eta_k. \] 
            \State Step: $x_{k+1}=R_{x_k}(\overline{\alpha}\beta^m\eta_k)$.
         \EndFor
      \end{algorithmic}
   \end{algorithm}
   
   If we take $t_k = 1/(2x_k^TAx_k)$, we recover the \emph{power method}.

\end{frame}


\begin{frame}{Higher-order Methods}
   ``Optimization Algorithms on Matrix Manifolds'' also discusses higher-order methods:
   \begin{itemize}
      \item Newton's method
      \item Trust-region methods
      \item quasi-Newton methods
      \item Conjugate gradients\\[1em]
   \end{itemize}

   Additional elements of differential geometry are required:
   \begin{itemize}
      \item Affine connections (e.g. Levi-Civita)
      \item Riemannian Hessian
      \item Vector transport
   \end{itemize}

\end{frame}
% ]]]


\section{Manopt}
% [[[
\begin{frame}{Manopt}{}
   {\sc matlab} toolbox developed by Nicolas Boumal and Bamdev Mishra, with many user contributions.
   \begin{center}
      \includegraphics[width=\textwidth]{figures/manopt_org.png}
   \end{center}
\end{frame}

%TODO highlight code, or just display only the portion we care about?  Ask Stephen.
%\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}
%
%{
%\begin{lstlisting}[language=Matlab,escapechar=!]
%% Generate symmetric matrix
%!\colorbox{yellow}{n = 1000;}!
%!\colorbox{yellow}{A = randn(n); A = 0.5*(A+A.');}!
%
%% Create problem structure
%!\colorbox{yellow}{M = spherefactory(n);}!
%!\colorbox{yellow}{problem.M = M;}!
%
%% Define cost function and Euclidean gradient
%problem.cost = @(x) -x'*(A*x);
%problem.egrad = @(x) -2*A*x;
%
%% Numerically check gradient consistency
%%checkgradient(problem);
%
%% Solve
%[x,xcost,info,opt] = trustregions(problem);
%\end{lstlisting}
%}
%
%\end{frame}

\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}
   
   %NOTE Manifolds in Manopt are described by structures.  There are "factory" functions that return
   %     these structures, and the fields of the structure provide common routines, such as ...
   \begin{lstlisting}[language=Matlab]
% Generate symmetric matrix
n = 1000;
A = randn(n); A = 0.5*(A+A.');

% Create problem structure
M = spherefactory(n);
problem.M = M;
   \end{lstlisting}

\end{frame}

\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}

{\tiny
\begin{verbatim}
>> M

M = 

                   name: @()sprintf('Sphere S^%d',n-1)
                    dim: @()n*m-1
                  inner: @(x,d1,d2)d1(:).'*d2(:)
                   norm: @(x,d)norm(d,'fro')
                   dist: @(x,y)real(acos(x(:).'*y(:)))
            typicaldist: @()pi
                   proj: @(x,d)d-x*(x(:).'*d(:))
                tangent: @(x,d)d-x*(x(:).'*d(:))
            egrad2rgrad: @(x,d)d-x*(x(:).'*d(:))
            ehess2rhess: @spherefactory/ehess2rhess
                    exp: @exponential
                   retr: @retraction
                    log: @spherefactory/logarithm
                   hash: @(x)['z',hashmd5(x(:))]
                   rand: @()random(n,m)
                randvec: @(x)randomvec(n,m,x)
                lincomb: @matrixlincomb
                zerovec: @(x)zeros(n,m)
                 transp: @(x1,x2,d)M.proj(x2,d)
               pairmean: @spherefactory/pairmean
                    vec: @(x,u_mat)u_mat(:)
                    mat: @(x,u_vec)reshape(u_vec,[n,m])
    vecmatareisometries: @()true
\end{verbatim} 
}
\end{frame}

\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}

{
\begin{lstlisting}[language=Matlab,escapechar=!]
% Generate symmetric matrix
n = 1000;
A = randn(n); A = 0.5*(A+A.');

% Create problem structure
M = spherefactory(n);
problem.M = M;

% Define cost function and Euclidean gradient
problem.cost = @(x) -x'*(A*x);
problem.egrad = @(x) -2*A*x;

% Numerically check gradient consistency
checkgradient(problem);
\end{lstlisting}
}

\end{frame}

\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}
   Manopt has tools to check gradients, Hessians, etc.
   \begin{center}
      \includegraphics[width=\textwidth]{figures/rayleigh_sphere_gradcheck.pdf}
   \end{center}
\end{frame}

\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}

%NOTE The x coming out here is the top (algebraic) eigenvector
{
\begin{lstlisting}[language=Matlab,escapechar=!]
% Generate symmetric matrix
n = 1000;
A = randn(n); A = 0.5*(A+A.');

% Create problem structure
M = spherefactory(n);
problem.M = M;

% Define cost function and Euclidean gradient
problem.cost = @(x) -x'*(A*x);
problem.egrad = @(x) -2*A*x;

% Numerically check gradient consistency
%checkgradient(problem);

% Solve
[x,xcost,info,opt] = trustregions(problem);
\end{lstlisting}
}

\end{frame}

\begin{frame}[fragile]{Manopt - Rayleigh Quotient on $S^{n-1}$}
   \begin{center}
      \includegraphics[width=\textwidth]{figures/rayleigh_sphere_gradnorm.pdf}
   \end{center}
\end{frame}



\begin{frame}[fragile]{Manopt - Generalized Rayleigh Quotient on $\text{Grass}(p,n)$}
%NOTE The Y coming out here is spans the invariant subspace corresponding to the 
%     largest (algebraic) p eigenvalues.  Internally to Manopt, Y is stored as an ON matrix;
%     this is done for efficiency, accuracy, and convenience.
\begin{lstlisting}[language=Matlab,escapechar=!]
% Generate symmetric matrix
n = 1000;
A = randn(n); A = 0.5*(A+A.');

% Create problem structure
p = 5;
M = grassmannfactory(n,p);
problem.M = M;

% Define cost function and Riemannian gradient
% In grassmannfactory, Y is an ON matrix
problem.cost = @(Y) -trace(Y'*A*Y);
problem.grad = @(Y) -2*(A*Y - Y*(Y'*A*Y));

% Numerically check gradient consistency
%checkgradient(problem);

% Solve
[Y,Ycost,info,opt] = trustregions(problem);
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Manopt - Generalized Rayleigh Quotient on $\text{Grass}(p,n)$}
   \begin{center}
      \includegraphics[width=\textwidth]{figures/rayleigh_grassmann_gradnorm.pdf}
   \end{center}
\end{frame}

%TODO this slide isn't complete
%\begin{frame}{Manopt}
%   Manopt is modular:
%   \begin{itemize}
%      \item Manifolds (55 currently implemented)
%      \item Solvers (GD, CG, Trust-region, etc.)
%      \item Tools (gradient check)
%   \end{itemize}
%\end{frame}

% ]]]


%TODO g-convex result from S. Sra + R. Hosseini's paper?
%TODO low-rank matrix completion stuff?
\section{Some Recent Work}
% [[[
\begin{frame}{Geodesic Convexity for GMMs}
   Recent work by Suvrit Sra and Reshad Hosseini.\\

   GMM loss is non-convex.

   \begin{center}
      \includegraphics[width=.8\textwidth]{figures/gmm.png}
   \end{center}
\end{frame}

\begin{frame}{Geodesic Convexity for GMMs}
   \begin{definition}[g-convex sets]
      A set $\mathcal{X}\subset \mathcal{M}$ is \emph{geodesically convex} if any two points of $\mathcal{X}$ are joined by a geodesic in $\mathcal{X}$.
   \end{definition}
   \begin{definition}[g-convex functions]
      Let $\mathcal{X}\subset \mathcal{M}$ be g-convex.  A function $f:\mathcal{X}\to\reals$ is \emph{geodesically convex} if $\forall x,y\in\mathcal{M}$,

      \[ f(\gamma(t)) \le (1-t) f(x) + tf(y), \] 

      \noindent where $\gamma:[0,1]\to\mathcal{X}$ is a geodesic connecting $x$ and $y$.
   \end{definition}

\end{frame}

\begin{frame}{Geodesic Convexity for GMMs}
   Can reparameterize GMM loss over manifold of positive-definite matrices so fitting a \emph{single} Gaussian \emph{is} g-convex.\\
   \emph{GMM loss is still not g-convex}.\\
   However, there \emph{is} a substantial benefit (competitive with EM).

   \begin{center}
      \includegraphics[width=1\textwidth]{figures/gmm_loss.pdf}
   \end{center}
\end{frame}

% ]]]


\begin{frame}{Thanks!}
   
   \begin{center}
      \includegraphics[width=0.5\textwidth]{figures/torus.png}
   \end{center}
   
   References:
   \begin{itemize}
      \item P.-A. Absil, R. Mahoney, R. Sepulchre, \emph{Optimization Algorithms on Matrix Manifolds}, Princeton University Press (2008).
      \item N. Boumal, B. Mishra, P.-A. Absil, R. Sepulchre, \emph{Manopt: a Matlab Toolbox for Optimization on Manifolds}, JMLR (2014).
      \item R. Hosseini, S. Sra, \emph{Manifold Optimization for Gaussian Mixture Models}, Arxiv:1506.07677 (2015).
   \end{itemize}
\end{frame}



\begin{frame}{Pictures}
   I took various pictures from
   
   \begin{itemize}
      \item ``Optimization Algorithms on Matrix Manifolds''
      \item ``Optimization on Manifolds: Methods and Applications'', P.-A. Absil at British-French-German Conference on Optimization, 2009.
      \item Wikipedia
   \end{itemize}

\end{frame}

% [[[ OLD TALK
%\begin{frame}{Outline}
%   \begin{enumerate}
%      \item A generic optimiztion problem
%      \item Example 1: Image Deblurring
%      \item Example 2: Blind Channel Estimation
%   \end{enumerate}
%
%   \begin{center}
%   \begin{columns}[b]
%      \begin{column}{0.45\textwidth}
%         \begin{center}
%         \includegraphics[width=0.8\textwidth]{../ieee_spm/figures/cameraman_observed_trim.pdf}
%         \end{center}
%      \end{column}
%      
%      \begin{column}{0.45\textwidth}
%         \begin{center}
%         \includegraphics[width=0.8\textwidth]{../ieee_spm/figures/{cameraman_rec_200_bior4.4_sym_trim}.pdf}
%         \end{center}
%      \end{column}
%   \end{columns}
%   \end{center}
%
%
%\end{frame}
%
%\section{Generic Problem}
%% [[[
%\begin{frame}{A Generic Problem}
%   Consider
%
%   \[ \min_x \dfrac{1}{2}\|\mathcal{A}x-b\|_2^2 + \lambda \|x\|_1. \] 
%
%   \begin{itemize}
%      \item $\mathcal{A}$ is a linear operator on problem variable $x$.
%      \item $b$ is measured data (e.g. blurry image).
%      \item Include $\lambda \|x\|_1$, to induce sparsity in $x$ (hopefully).
%   \end{itemize}
%
%   Define 
%
%   \[ f(x) = \dfrac{1}{2}\|\mathcal{A}x-b\|_2^2, \quad g(x) = \lambda \|x\|_1. \] 
%
%   \noindent $f(x)$ is convex, differentiable, $g(x)$ is convex, non-differentiable.
%   
%   \[ \nabla f(x) = \mathcal{A}^\ast\left(\mathcal{A}x-b\right). \] 
%
%\end{frame}
%
%\begin{frame}{Proximal Gradient Method}
%   \[ \min_x \dfrac{1}{2}\|\mathcal{A}x-b\|_2^2 + \lambda \|x\|_1. \] 
%
%   \[ \nabla f(x) = \mathcal{A}^\ast\left(\mathcal{A}x-b\right), \quad x^{+} = \operatorname{prox}_{t g}\left(x - t\nabla f(x)\right). \] 
%
%   \noindent Need to efficiently compute
%   \begin{itemize}
%      \item $\operatorname{prox}_{tg}(x)$ with $g(x) = \lambda \|x\|_1$.  ``Shrinkage'' is fast.
%      \item $\mathcal{A}$.  Usually have fast forward and inverse transform (e.g. FFT, discrete wavelet transform).
%      \item $\mathcal{A}^\ast$.  Sometimes not so easy...  Let's look at a couple examples.
%   \end{itemize}
%\end{frame}
%
%% ]]]
%
%
%\section{Image Deblurring}
%% [[[
%\begin{frame}{Image Deblurring Problem}
%   Observation: natural images tend to have sparse wavelet coefficients.\\
%   
%   \begin{itemize}
%      \item $b$ - observed blurred image, with known blurring operator $\mathcal{R}$\\ (e.g. Gaussian PSF applied efficiently in Fourier domain)
%      \item $\mathcal{W}$ - multi-level wavelet synthesis operator
%      \item $x$ - wavelet coefficients
%   \end{itemize}
%
%   Natural problem formulation is
%
%   \[ \min_x \dfrac{1}{2}\left\|\mathcal{RW}x-b\right\|_2^2 + \lambda \|x\|_1. \]
%
%   \[ \nabla f(x) = \mathcal{W}^\ast\mathcal{R}^\ast\left(\mathcal{RW}x-b\right). \] 
%
%\end{frame}
%
%\begin{frame}{Image Deblurring Problem}
%   \begin{center}
%   \begin{columns}[t]
%      \begin{column}{0.45\textwidth}
%         Observed image:
%         \includegraphics[width=\textwidth]{../ieee_spm/figures/cameraman_observed_trim.pdf}
%      \end{column}
%      
%      \begin{column}{0.45\textwidth}
%         Recovered image:
%         \includegraphics[width=\textwidth]{../ieee_spm/figures/{cameraman_rec_200_bior4.4_sym_trim}.pdf}
%      \end{column}
%   \end{columns}
%   \end{center}
%\end{frame}
%
%\begin{frame}{Adjoint of Wavelet Operator}
%   \[ \nabla f(x) = \mathcal{W}^\ast\mathcal{R}^\ast\left(\mathcal{RW}x-b\right). \] 
%   
%   \begin{itemize}
%      \item $\mathcal{W}$ is wavelet synthesis (reconstruction).  Standard routine in libraries.
%      % actually DCT is used for symmetric/reflexive BCs
%      \item $\mathcal{R}$ and $\mathcal{R}^\ast$ for blurring PSF can be applied rapidly in Fourier domain (FFT).
%      \item What about $\mathcal{W}^\ast$?  Not a standard operation like $\mathcal{W}$ and $\mathcal{W}^\dagger$.
%   \end{itemize}
%
%   If $\mathcal{W}$ is orthogonal, $\mathcal{W}^\ast = \mathcal{W}^\dagger$.\\
%   If $\mathcal{W}$ is biorthogonal, $\mathcal{W}^\ast \approx \mathcal{W}^\dagger$.\\
%   So, one option is
%
%   \[ \nabla f(x) \approx \mathcal{W}^\dagger\mathcal{R}^\ast\left(\mathcal{RW}x-b\right). \]
%   
%   % And many people use this approach.  It actually works pretty well.  But how much error does W^* = W^+ indtroduct?
%\end{frame}
%
%\begin{frame}{Adjoint of Wavelet Operator}
%   Digging around in frame theory a bit, it turns out ${\color{blue} \mathcal{W}^\ast = \tilde{\mathcal{W}}^\dagger}$: {\color{blue}the adjoint of wavelet synthesis is dual wavelet analysis}.\\[1em]
%
%   But we must also handle boundary conditions.  This is usually done by extending the signal via $\mathcal{E}$ to satisfy the BCs.\\[1em]
%
%   The relation $\mathcal{W}^\ast = \tilde{\mathcal{W}}^\dagger$ holds for $\mathcal{E}$ being zero-padding, since $\mathcal{E}_\text{zpd}^\ast = \mathcal{E}_\text{zpd}^\dagger$.  Let $\mathcal{W}_\text{zpd}$ be wavelet synthesis with zero BCs.\\[1em]
%
%   For general $\mathcal{E}$, we have\\[-1em]
%
%   \[ \mathcal{W}^\dagger = \mathcal{W}_\text{zpd}^\dagger\mathcal{E} \implies \mathcal{W}=\mathcal{E}^\dagger\mathcal{W}_\text{zpd} \implies \mathcal{W}^\ast = \mathcal{W}_\text{zpd}^\ast(\mathcal{E}^\dagger)^\ast \]
%
%   and\\[-1em]
%
%   \[ {\color{blue} \mathcal{W}^\ast = \tilde{\mathcal{W}}_\text{zpd}^\dagger (\mathcal{E}^\dagger)^\ast}. \] 
%
%
%\end{frame}
%
%\begin{frame}{Adjoint of Pseudoinverse Extension}
%   % Throughout all of this, we're trying to leverage the fact that we already have a library to compute W, W^+, etc.
%   Now we just need to implement $(\mathcal{E}^\dagger)^\ast$!  $\tilde{\mathcal{W}}_\text{zpd}^\dagger$ is a standard and fast operation.\\[.5em]
%
%   Consider a signal $y[n]$, $n=0,...,N-1$.  Let $L_p$ be the length of wavelet analysis filters.\\[.5em]
%
%   Zero padding:
%   \[ \underbrace{0, ..., 0}_{L_p-1}, y[0], ..., y[N-1], \underbrace{0, ..., 0}_{L_p-1}. \]
%
%   Half-point symmetric:
%   \[ \underbrace{y[L_p-1], ..., y[0]}_\text{Left extension}, y[0], ..., y[N-1], \underbrace{y[N-1], ..., y[N+L_p-2]}_\text{Right extension}. \] 
%
%\end{frame}
%
%\begin{frame}{Zero padding}
%   Zero padding as a linear operator:
%
%   \begin{columns}
%      \begin{column}{0.7\textwidth}
%\[ \mathcal{E}_\text{zpd} = \begin{bmatrix} 0_{(L_p-1)\times N}\\ I_{N\times N}\\ 0_{(L_p-1)\times N}\end{bmatrix} = \begin{bmatrix} 0 & 0 & \cdots & 0 & 0\\ \vdots & \vdots &\ddots & \vdots & \vdots\\ 0 & 0 & \cdots & 0 & 0\\[0.5em] 1 & 0 & \cdots & 0 & 0\\ 0 & 1 & \cdots & 0 & 0\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ 0 & 0 & \cdots & 1 & 0\\ 0 & 0 & \cdots & 0 & 1\\[0.5em] 0 & 0 & \cdots & 0 & 0\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ 0 & 0 & \cdots & 0 & 0\end{bmatrix}. \] 
%      \end{column}
%
%      \begin{column}{0.3\textwidth}
%         In this case
%
%         \[ \left(\mathcal{E}_\text{zpd}^\dagger\right)^\ast = \mathcal{E}_\text{zpd}. \] 
%
%         \noindent This is what allows us the factorization
%
%         \[ \mathcal{W}^\ast = \tilde{\mathcal{W}}_\text{zpd}^\dagger \left(\mathcal{E}^\dagger\right)^\ast. \] 
%
%
%      \end{column}
%   \end{columns}
%\end{frame}
%
%\begin{frame}{Half-point symmetric}
%   Half-point symmetric extension as a linear operator:
%
%\[ \scalemath{0.75}{\mathcal{E}_\text{sym} = \begin{bmatrix} & & \iddots & & &\\ & 1 &&&&\\ 1&&&&&\\1&&&&\\&1&&&&\\&&\ddots&&&\\&&&1&\\&&&&1\\&&&&1\\&&&1&\\&&\iddots&&\end{bmatrix}, \quad \left(\mathcal{E}_\text{sym}^\dagger\right)^\ast = \scalemath{0.75}{\begin{bmatrix} & \iddots \\ 1/2&&&&&\\1/2&&&&\\&\ddots&&&&&&\\&&1/2&\\&&&1\\&&&&\ddots\\&&&&&1\\&&&&&&1/2\\&&&&&&&\ddots\\&&&&&&&&1/2\\&&&&&&&&1/2\\&&&&&&&\iddots\\\end{bmatrix}}}. \]
%
%\end{frame}
%
%\begin{frame}{Image Deblurring Problem}
%   $200$ iterations of FISTA, $\mathcal{W}^\dagger$ is a 3-stage CDF 9/7 wavelet transform, ${\lambda = 2\times 10^{-5}}$
%   \begin{center}
%   \begin{columns}[t]
%      \begin{column}{0.45\textwidth}
%         Using $\mathcal{W}^\ast \approx \mathcal{W}^\dagger$:
%         \includegraphics[width=\textwidth]{../ieee_spm/figures/{cameraman_rec_200_bior4.4_sym_badjoint_trim}.pdf}
%
%         \[ \scalemath{0.75}{\dfrac{\|\mathcal{W}x-y\|_2}{\|y\|_2} = 7.25\times 10^{-2}} \] 
%      \end{column}
%      
%      \begin{column}{0.45\textwidth}
%         Using $\mathcal{W}^\ast = \tilde{\mathcal{W}}_\text{zpd}^\dagger(\mathcal{E}^\dagger)^\ast$:
%         \includegraphics[width=\textwidth]{../ieee_spm/figures/{cameraman_rec_200_bior4.4_sym_trim}.pdf}
%         \[ \scalemath{0.75}{\dfrac{\|\mathcal{W}x-y\|_2}{\|y\|_2} = 7.24\times 10^{-2}} \] 
%      \end{column}
%   \end{columns}
%   \end{center}
%
%\end{frame}
%
%\begin{frame}{Image Deblurring Problem}
%   \begin{itemize}
%      \item $\mathcal{W}^\ast = \tilde{\mathcal{W}}_\text{zpd}^\dagger(\mathcal{E}^\dagger)^\ast$.
%      \item $\tilde{\mathcal{W}}_\text{zpd}^\dagger$ is standard in wavelet libraries.
%      \item $(\mathcal{E}^\dagger)^\ast$ is closed-form (once you find it) and fast.
%      \item So we can apply $\mathcal{W}^\ast$ efficiently \emph{and correctly} in
%
%         \[ \nabla f(x) = \mathcal{W}^\ast\mathcal{R}^\ast\left(\mathcal{RW}x-b\right). \] 
%   \end{itemize}
%\end{frame}
%
%% ]]]
%
%
%\section{Blind Channel Estimation}
%% [[[
%\begin{frame}{BCE Problem}
%   %TODO could change output signals to y_i and y?  Doesn't match BCE literature, but works better with above
%   Unknown source sends signal $s$ over unknown channels with impulse responses $h_i$.  We observe channel outputs
%
%   \[ x_i[n] = \{h_i\ast s\}[n]. \] 
%   
%   \noindent Can we recover source and channel IRs?\\
%
%   Let's restrict ourselves to $h_i$ and $s$ real-valued.
%
%   Notice that for any $\alpha\neq 0$, 
%
%   \[ x_i[n] = \{h_i\ast s\}[n] = \{\alpha h_i \ast \dfrac{1}{\alpha} s\}[n]. \] 
%
%   \noindent So we can maybe recover $h_i$ and $s$ up to a factor.
%
%\end{frame}
%
%\begin{frame}{BCE Problem}
%   For simplicity of notation, consider a single channel $h$ with output $x$.  Let $h$ be of length $K$ and $s$ be of length $N$; $x$ will be of length $K+N-1$.\\[0.5em]
%
%   One can write the convolution as linear operator on the $K\times N$ matrix $hs^T$:
%
%   \[ x = h\ast s = \mathcal{A}(hs^T). \] 
%
%   \noindent Assuming $h$ and $s$ should be sparse in time, a natural problem formulation is
%      
%   \[ \min_{h,s} \dfrac{1}{2}\|\mathcal{A}(hs^T)-x\|_2^2 + \lambda_h\|h\|_1 + \lambda_s\|s\|_1. \] 
%
%   \noindent This is non-convex.  Can use other regularization terms (e.g. $\|h\|_\text{TV}$).
%\end{frame}
%
%\begin{frame}{Adjoint of $\mathcal{A}$}
%   Define $f(h,s) = 1/2\|\mathcal{A}(hs^T)-x\|_2^2$.\\[.5em]
%   
%   The required gradients are
%   \begin{align*}
%      \nabla_h f(h,s) &= \left[\mathcal{A}^\ast(\mathcal{A}(hs^T)-x)\right]s\\
%      \nabla_s f(h,s) &= \left[\mathcal{A}^\ast(\mathcal{A}(hs^T)-x)\right]^Th.
%   \end{align*}
%
%   Note that $\mathcal{A}$ takes a matrix and returns a vector.  So $\mathcal{A}^\ast$ must take a vector and return a matrix (of appropriate size).\\
%
%\end{frame}
%
%\begin{frame}{Adjoint of $\mathcal{A}$}
%   We know the action of $\mathcal{A}(hs^T)$:
%      
%   \[ x[n] =  \sum_{k=k_1(n)}^{k_2(n)} h[k]s[n-k], \] 
%
%   where $k_1(n) = \max\{0,n+1-N\}$ and $k_2(n) = \min\{K-1,n\}$.
%
%\end{frame}
%
%\begin{frame}{Adjoint of $\mathcal{A}$}
%   We know the action of $\mathcal{A}(hs^T)$:
%
%   \begin{center}
%   \begin{tikzpicture}[baseline=(hs.center), ampersand replacement=\&]
%      % [[[
%      % based on figure from Romberg et al. "Multichannel blind deconvolution..."
%      % and examples from http://www.texample.net/tikz/examples/mnemonic-rule-for-matrix-determinant/
%      % http://tex.stackexchange.com/questions/83046/position-two-stacked-tikz-matrices
%      % http://www.texample.net/tikz/examples/energy-level-diagram/
%      \matrix (hs) [matrix of math nodes, nodes = {node style ge},, column sep=0mm, row sep=-8mm]
%      {h[0]s[0] \& h[0]s[1] \& h[0]s[2] \& h[0]s[3] \\
%       h[1]s[0] \& h[1]s[1] \& h[1]s[2] \& h[1]s[3] \\
%       h[2]s[0] \& h[2]s[1] \& h[2]s[2] \& h[2]s[3] \\
%       h[3]s[0] \& h[3]s[1] \& h[3]s[2] \& h[3]s[3] \\
%       h[4]s[0] \& h[4]s[1] \& h[4]s[2] \& h[4]s[3] \\
%       h[5]s[0] \& h[5]s[1] \& h[5]s[2] \& h[5]s[3] \\
%       h[6]s[0] \& h[6]s[1] \& h[6]s[2] \& h[6]s[3] \\
%       \vdots \& \vdots \& \vdots \& \vdots\\
%      };
%   
%      \begin{scope}[xshift=-35mm,yshift=-2.5mm]
%      \matrix (y) [matrix of math nodes, nodes = {node style ge},, column sep=0mm, row sep=-3mm]
%      {x[0]\\
%       x[1]\\
%       x[2]\\
%       x[3]\\
%       x[4]\\
%       x[5]\\[-2mm]
%       \vdots\\
%      };
%      \end{scope}
%   
%      \draw [thick,->,>=stealth,shorten >=4mm,shorten <=-4mm,color=blue] (hs-1-1.center) to ($(y-1-1.center) + (0mm,-1mm)$); % this is a hack, but it's quite close
%      \draw [thick,->,>=stealth,shorten >=-9mm,shorten <=-4mm,color=blue] (hs-1-2.center) to (hs-2-1.center);
%      \draw [thick,->,>=stealth,shorten >=-9mm,shorten <=-4mm,color=blue] (hs-1-3.center) to (hs-3-1.center);
%      \draw [thick,->,>=stealth,shorten >=-9mm,shorten <=-4mm,color=blue] (hs-1-4.center) to (hs-4-1.center);
%      \draw [thick,->,>=stealth,shorten >=-9mm,shorten <=-4mm,color=blue] (hs-2-4.center) to (hs-5-1.center);
%      \draw [thick,->,>=stealth,shorten >=-9mm,shorten <=-4mm,color=blue] (hs-3-4.center) to (hs-6-1.center);
%      % ]]]
%   \end{tikzpicture}
%   \end{center}
%
%\end{frame}
%
%\begin{frame}{Adjoint of $\mathcal{A}$}
%   Adjoint is defined via
%   \[ \left\langle \mathcal{A}(X), y \right\rangle = \langle X,\mathcal{A}^\ast(y)\rangle \quad \forall X \forall y. \] 
%   
%   Plug in explicit form of $\mathcal{A}(X)$:
%
%   \begin{align*}
%   \left\langle \mathcal{A}(X),y\right\rangle &= \sum_{n=0}^{K+N-2}y[n] \left(\sum_{k=k_1(n)}^{k_2(n)} X[k,n-k]\right)\\
%                                              &= y[0]X[0,0] + y[1]\left(X[0,1]+X[1,0]\right)\\
%                                              &\,+ y[2]\left(X[0,2] + X[1,1] + X[2,0]\right) + \cdots.\\
%   \end{align*}
%
%   Notice that $X[i,j]$ is always multiplied by $y[i+j]$.  Defines Hankel matrix!
%
%\end{frame}
%
%\begin{frame}{Adjoint of $\mathcal{A}$}
%   \begin{align*}
%      \left\langle \mathcal{A}(X),y\right\rangle &= \sum_{n=0}^{K+N-2}y[n] \left(\sum_{k=k_1(n)}^{k_2(n)} X[k,n-k]\right)\\
%                                                 &= \sum_{i=0}^{K-1}\sum_{j=0}^{N-1} X[i,j]y[i+j]\\
%                                                 &= \sum_{i=0}^{K-1}\sum_{j=0}^{N-1} X[i,j]Y[i,j]\\
%                                                 &= \left\langle X, Y\right\rangle = \left\langle X, \mathcal{A}^\ast(y)\right\rangle,
%   \end{align*}
%
%   where we define the $K\times N$ Hankel matrix $Y$ by $Y[i,j] = y[i+j]$ so
%
%   \[ \mathcal{A}^\ast(y) = Y. \] 
%
%\end{frame}
%
%\begin{frame}{Hankel matrix-vector product}
%   The Hankel matrix $Y$ is dense but structured:
%
%   \[ Y = \begin{bmatrix} y[0] & y[1] & y[2] & y[3] & y[4]\\
%                          y[1] & y[2] & y[3] & y[4] & y[5]\\
%                          y[2] & y[3] & y[4] & y[5] & y[6]\\\end{bmatrix}. \] 
%   
%   Reorder columns to get Toeplitz matrix:
%   \[\scalemath{0.9}{ T = YP = \begin{bmatrix}
%                               y[4] & y[3] & y[2] & y[1] & y[0]\\
%                               y[5] & y[4] & y[3] & y[2] & y[1]\\
%                               y[6] & y[5] & y[4] & y[3] & y[2]\\\end{bmatrix}, 
%              \quad P = \begin{bmatrix} 0 & 0 & 0 & 0 & 1\\
%                                        0 & 0 & 0 & 1 & 0\\
%                                        0 & 0 & 1 & 0 & 0\\
%                                        0 & 1 & 0 & 0 & 0\\
%                                        1 & 0 & 0 & 0 & 0\\ \end{bmatrix}. }\] 
%\end{frame}
%
%\begin{frame}{Hankel matrix-vector product}
%   Embed Toeplitz matrix in circulant matrix:
%   
%   % http://tex.stackexchange.com/questions/33519/vertical-line-in-matrix-using-latexit
%   \[ C = \left[\begin{array}{@{}ccccc|cc@{}}
%            y[4] & y[3] & y[2] & y[1] & y[0] & y[6] & y[5]\\
%            y[5] & y[4] & y[3] & y[2] & y[1] & y[0] & y[6]\\
%            y[6] & y[5] & y[4] & y[3] & y[2] & y[1] & y[0]\\
%            \hline
%            y[0] & y[6] & y[5] & y[4] & y[3] & y[2] & y[1]\\
%            y[1] & y[0] & y[6] & y[5] & y[4] & y[3] & y[2]\\
%            y[2] & y[1] & y[0] & y[6] & y[5] & y[4] & y[3]\\
%            y[3] & y[2] & y[1] & y[0] & y[6] & y[5] & y[4]\\
%   \end{array}\right]. \] 
%
%   \noindent We can do a fast circulant mat-vec via the FFT!  Thus, we can compute and apply $\mathcal{A}^\ast(y)$ rapidly.
%
%\end{frame}
%
%%XXX this recovery is cherry picked!  Lots of parameters to tune!
%\begin{frame}{Example estimation}
%   Still a work in progress!\\
%   \begin{center}
%      \includegraphics[width=\textwidth]{figures/bce_rec_02_h_trim.pdf}
%   \end{center}
%\end{frame}
%
%\begin{frame}{Example estimation}
%   Still a work in progress!\\
%   \begin{center}
%      \includegraphics[width=\textwidth]{figures/bce_rec_02_s_trim.pdf}
%   \end{center}
%\end{frame}
%
%
%% ]]]
%
%\begin{frame}{Thanks!}
%   We can now compute the adjoint wavelet transform:
%
%   \[ \mathcal{W}^\ast = \tilde{\mathcal{W}}_\text{zpd}^\dagger(\mathcal{E}^\dagger)^\ast. \] 
%   
%   Interesting adjoint in blind channel estimation problem:
%
%   \[ x = h\ast s = \mathcal{A}(hs^T). \] 
%   References:
%   \begin{itemize}
%      \item A. Beck, M. Teboulle, \emph{A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems}, SIAM Journal on Imaging Science, (2009).
%      \item A. Ahmed, B. Recht, J. Romberg, \emph{Blind Deconvolution using Convex Programming}, IEEE Trans. on Info. Theory, (2013).
%   \end{itemize}
%\end{frame}
%
%%XXX Extra slides
%
%% some proximal gradient details
%% [[[
%\begin{frame}{Proximal Gradient Method}
%   \[ f(x) = \dfrac{1}{2}\|\mathcal{A}x-b\|_2^2, \quad g(x) = \lambda \|x\|_1. \] 
%
%   We can use a variant of simple gradient descent to solve the generic problem
%
%   \[ \min_x f(x) + g(x). \] 
%
%   %XXX dropping k step notation for brevity.  x^+ is the new x after the prox grad step
%   Proximal gradient method:
%   \[ x^{+} = \operatorname{prox}_{t g}\left(x - t\nabla f(x)\right), \text{ step size $t>0$}. \] 
%   
%   Proximity function: 
%   \[ \operatorname{prox}_g(x) \defeq \argmin_u\left(g(u) + \dfrac{1}{2}\|u-x\|_2^2\right). \] 
%
%\end{frame}
%
%\begin{frame}{Proximal Gradient Method}
%   %\[ x^{(k)} = \operatorname{prox}_{t_k g}\left(x^{(k-1)} - t_k\nabla f(x^{(k-1)})\right). \] 
%   \[ \operatorname{prox}_g(x) \defeq \argmin_u\left(g(u) + \dfrac{1}{2}\|u-x\|_2^2\right). \] 
%
%   For $g(x) = \lambda\|x\|_1$, proximity operator is ``shrinkage'':
%
%   \[ \left\{\operatorname{prox}_g(x)\right\}_i = \left\{\begin{array}{ll} x_i-\lambda & x_i \ge \lambda\\0 & |x_i| < \lambda\\ x_i + \lambda & x_i\le -\lambda\end{array}\right.. \] 
%
%   %XXX dropping k step notation for brevity.  x^+ is the new x after the prox grad step
%   Proximal gradient step minimizes $g(u)$ plus quadratic local model of $f(u)$ about $x$:
%
%   \begin{align*}
%      x^+ &= \operatorname{prox}_{t g}\left(x - t\nabla f(x)\right)\\
%              %&= \argmin_u\left(g(u) + \dfrac{1}{2t}\left\|u-x+t\nabla f(x)\right\|_2^2\right)\\
%              &= \argmin_u\left(g(u) + f(x) + \left\langle \nabla f(x), u-x\right\rangle + \dfrac{1}{2t}\|u-x\|_2^2\right).
%   \end{align*}
%
%\end{frame}
%% ]]]
%
%
%% proximal gradient and FISTA side-by-side
%% [[[
%\begin{frame}{Proximal Gradient and FISTA}
%   Proximal gradient:\\
%   \begin{itemize}
%      \item Choose $x^{(0)}$.
%      \item For $k=1,2,...$\\[-1em]
%
%         \[ x^{(k)} = \operatorname{prox}_{t_k g}\left(x^{(k-1)} - t_k\nabla f(x^{(k-1)})\right), \text{ with step size $t_k$.} \] 
%   \end{itemize} 
%
%   FISTA (fast iterative shrinkage-thresholding algorithm):\\
%   \begin{itemize}
%      \item Choose $x^{(0)}=x^{(-1)}$.
%      \item For $k=1,2,...$\\[-1em]
%         \[ y = x^{(k-1)} + \dfrac{k-2}{k+1}\left(x^{(k-1)}-x^{(k-2)}\right) \] 
%         \[ x^{(k)} = \operatorname{prox}_{t_kg}\left(y - t_k\nabla f(y)\right), \text{ with step size $t_k$.} \] 
%   \end{itemize}
%
%\end{frame}
%% ]]]
%
%
%% Some frame theory
%% [[[
%\begin{frame}{Adjoint of Wavelet Operator}
%   We could use $\mathcal{W}^\ast \approx \mathcal{W}^\dagger$, but it turns out we can find the adjoint exactly!\\
%   
%   \begin{itemize}
%      \item Related to $\mathcal{W}$ are frame vectors $\phi_n$ (the wavelet basis vectors), which define a frame operator $\Phi=\mathcal{W}^\dagger$:
%
%         \[ \Phi f[n] = \langle f, \phi_n\rangle. \] 
%      \item We can define the dual frame vectors $\tilde{\phi}_n=\left(\Phi^\ast\Phi\right)^{-1}\phi_n$.
%      \item Define the dual frame operator via
%
%         \[ \tilde{\Phi}f[n] = \langle f, \tilde{\phi}_n\rangle. \] 
%
%      \item Digging around in frame theory a bit, we find
%
%         \[ \Phi^\ast = \tilde{\Phi}^\dagger \implies {\color{blue}\mathcal{W}^\ast = \tilde{\mathcal{W}}^\dagger}\] 
%   \end{itemize}
%
%\end{frame}
%% ]]]
%
%
%% analysis formulation
%% [[[
%\begin{frame}{Analysis formulation}
%   \begin{columns}
%      \begin{column}{0.5\textwidth}
%         Synthesis formulation\\[-1em]
%         \[ \min_x \dfrac{1}{2}\|\mathcal{RW}x-b\|_2^2 + \lambda \|x\|_1 \]
%
%         $x$ contains coefficients.\\
%      \end{column}
%
%      \begin{column}{0.5\textwidth}
%         Analysis formulation\\[-1em]
%         \[ \min_y \dfrac{1}{2}\|\mathcal{R}y-b\|_2^2 + \lambda \|\mathcal{W}^\dagger y\|_1 \] 
%
%         $y$ is an image.
%      \end{column}
%   \end{columns}
%   
%   ~\\
%   In the analysis formulation, we need $\operatorname{prox}_g(y)$ with $g(y) = \lambda\|\mathcal{W}^\dagger y\|_1$ instead of just $\lambda\|x\|_1$.\\[1em]
%
%   We know how to compute the prox function if $\mathcal{W}^\dagger(\mathcal{W}^\dagger)^\ast = \nu I$.  This is okay for orthogonal wavelets, but not for biorthogonal wavelets.  But it's ``close'' in practice.\\[1em]
%   
%   P. Combettes, J.-C. Pesquet, \emph{A Douglas-Rachford splitting approach to nonsmooth convex variational signal recovery}, IEEE Journal of Selected Topics in Signal Processing, (2007).
%\end{frame}
%% ]]]
%
%
%% original cameraman and recovered images side-by-side
%% [[[
%\begin{frame}{Image Deblurring Problem}
%   $200$ iterations of FISTA, $\mathcal{W}^\dagger$ is a 3-stage CDF 9/7 wavelet transform, ${\lambda = 2\times 10^{-5}}$
%   \begin{center}
%   \begin{columns}[t]
%      \begin{column}{0.45\textwidth}
%         Original image:
%         \includegraphics[width=\textwidth]{../ieee_spm/figures/{cameraman}.pdf}
%
%      \end{column}
%      
%      \begin{column}{0.45\textwidth}
%         Using $\mathcal{W}^\ast = \tilde{\mathcal{W}}_\text{zpd}^\dagger(\mathcal{E}^\dagger)^\ast$:
%         \includegraphics[width=\textwidth]{../ieee_spm/figures/{cameraman_rec_200_bior4.4_sym_trim}.pdf}
%         \[ \scalemath{0.75}{\dfrac{\|\mathcal{W}x-y\|_2}{\|y\|_2} = 7.24\times 10^{-2}} \] 
%      \end{column}
%   \end{columns}
%   \end{center}
%
%\end{frame}
%
%% ]]]
%
%
%% some extra BCE recoveries (more-or-less cherry picked!)
%% [[[
%\begin{frame}{Example estimation}
%   Still a work in progress!\\
%   \begin{center}
%      \includegraphics[width=\textwidth]{figures/bce_rec_03_h_trim.pdf}
%   \end{center}
%\end{frame}
%
%\begin{frame}{Example estimation}
%   Still a work in progress!\\
%   \begin{center}
%      \includegraphics[width=\textwidth]{figures/bce_rec_03_s_trim.pdf}
%   \end{center}
%\end{frame}
%% ]]]

% ]]]

\end{document}

% vim: set spell:
% vim: foldmarker=[[[,]]]

